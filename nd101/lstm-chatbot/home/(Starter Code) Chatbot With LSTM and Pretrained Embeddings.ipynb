{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## References\n",
    "1. Sequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215\n",
    "1. PyTorch Tutorials: Chatbot Tutorial https://pytorch.org/tutorials/beginner/chatbot_tutorial.html \n",
    "1. PyTorch Tutorials: NLP From Scratch: Translation with a Sequence to Sequence Network and Attention https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html \n",
    "1. Sequence to Sequence Learning with Neural Networks https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "1. Seq2Seq-Chatbot https://github.com/iJoud/Seq2Seq-Chatbot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Build your Vacabulry & create the Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1: Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def loadDF(path=None):\n",
    "    \"\"\"\n",
    "    You will use this function to load the dataset into a Pandas Dataframe for processing.\n",
    "    \"\"\"\n",
    "    train_iter, dev_iter = torchtext.datasets.SQuAD1(root=path, split=('train', 'dev')) \n",
    "\n",
    "    train_dict = [{'context': context, 'question': question, 'answer': answers[0], 'indices': index} for context, question, answers, index in train_iter]\n",
    "    train_df = pd.DataFrame(train_dict)\n",
    "\n",
    "    dev_dict = [{'context': context, 'question': question, 'answer': answers[0], 'indices': index} for context, question, answers, index in dev_iter]\n",
    "    dev_df = pd.DataFrame(dev_dict)\n",
    "\n",
    "    return train_df, dev_df\n",
    "  \n",
    "train_df, dev_df = loadDF('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? \n",
      "<  Saint Bernadette Soubirous \n",
      "\n",
      ">  What is in front of the Notre Dame Main Building? \n",
      "<  a copper statue of Christ \n",
      "\n",
      ">  The Basilica of the Sacred heart at Notre Dame is beside to which structure? \n",
      "<  the Main Building \n",
      "\n",
      ">  What is the Grotto at Notre Dame? \n",
      "<  a Marian place of prayer and reflection \n",
      "\n",
      ">  What sits on top of the Main Building at Notre Dame? \n",
      "<  a golden statue of the Virgin Mary \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5): # first 5 Q&A\n",
    "    print(\"> \", train_df.iloc[i].question, \"\\n< \", train_df.iloc[i].answer, \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-2: Define the Vocabulary Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = \"<pad>\"  # Used for padding short sentences\n",
    "SOS_token = \"<sos>\"  # Start-of-sentence token\n",
    "EOS_token = \"<eos>\"  # End-of-sentence token\n",
    "UNK_token = \"<unk>\"\n",
    "\n",
    "PAD_index = 0\n",
    "SOS_index = 1\n",
    "EOS_index = 2\n",
    "UNK_index = 3\n",
    "\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.index2word = {PAD_index: PAD_token, SOS_index: SOS_token, EOS_index: EOS_token, UNK_index: UNK_token}\n",
    "        self.word2index = {token: index for index, token in self.index2word.items()}\n",
    "        self.word2count = {PAD_token: 0, SOS_token: 0, EOS_token: 0, UNK_token: 0}\n",
    "        self.n_words = len(self.index2word)\n",
    "\n",
    "    # Turn a Unicode string to plain ASCII, thanks to\n",
    "    # https://stackoverflow.com/a/518232/2809427\n",
    "    def unicodeToAscii(self, s: str):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "      )\n",
    "\n",
    "    # Cleans our words before adding them\n",
    "    def cleanText(self, text):\n",
    "        test = self.unicodeToAscii(text)\n",
    "        text = ''.join([s for s in text if s not in string.punctuation])\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text.lower())\n",
    "        text = [stemmer.stem(w) for w in text]\n",
    "        return text\n",
    "\n",
    "    # Indexes words in our vocabulary\n",
    "    def addWord(self, word: str, count: int=1) -> bool:\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = count\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            return True\n",
    "        else:\n",
    "            self.word2count[word] += count\n",
    "            return False\n",
    "\n",
    "    def trim(self, min_count: int = 2):\n",
    "        words2keep = [\n",
    "            (word, count) for word, count in self.word2count.items() \n",
    "            if count >= min_count or word in (PAD_token, SOS_token, EOS_token, UNK_token)\n",
    "        ]\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.index2word = {}\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "        for word, count in words2keep:\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = count\n",
    "            self.n_words += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-3: Load the Vocabulary and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding words in questions to the vocabulary...\n",
      "\tAdding word 10000.\n",
      "\tAdding word 20000.\n",
      "Adding words in answers to the vocabulary...\n",
      "\tAdding word 10000.\n",
      "\tAdding word 20000.\n",
      "\tAdding word 30000.\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab()\n",
    "\n",
    "# Iterate through our vocabulary dataframe and add each word to the vocabulary.\n",
    "print(\"Adding words in questions to the vocabulary...\")\n",
    "count = vocab.n_words\n",
    "for i, r in train_df.iterrows():\n",
    "    text = vocab.cleanText(r.question)\n",
    "    for t in text:\n",
    "        if vocab.addWord(t):\n",
    "            if count % 10_000 == 0:\n",
    "                print(f\"\\tAdding word {count}.\")\n",
    "            count += 1\n",
    "\n",
    "vocab = Vocab()\n",
    "\n",
    "print(\"Adding words in answers to the vocabulary...\")\n",
    "count = vocab.n_words\n",
    "for i, r in train_df.iterrows():\n",
    "    text = vocab.cleanText(r.answer)\n",
    "    for t in text:\n",
    "        if vocab.addWord(t):\n",
    "            if count % 10_000 == 0:\n",
    "                print(f\"\\tAdding word {count}.\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the rare words decreases the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original token count: 33115\n",
      "reduced token count: 3890\n"
     ]
    }
   ],
   "source": [
    "print(\"original token count:\", vocab.n_words)\n",
    "vocab.trim(min_count=10)\n",
    "print(\"reduced token count:\", vocab.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_train_dataset, TRG_train_dataset  = train_df.question.values, train_df.answer.values\n",
    "SRC_test_dataset, TRG_test_dataset = dev_df.question.values, dev_df.answer.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 87599\n",
      "Number of testing samples: 10570\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(SRC_train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(SRC_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, vocab: Vocab):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        x = self.vocab.cleanText(x)\n",
    "        y = self.vocab.cleanText(y)\n",
    "\n",
    "        # Convert words to indeces\n",
    "        x = [self.vocab.word2index[w] for w in x if w in self.vocab.word2index] \n",
    "        y = [self.vocab.word2index[w] for w in y if w in self.vocab.word2index]\n",
    "\n",
    "        # Fill 0 to make the same length.\n",
    "        x = [SOS_index] + x + [EOS_index]\n",
    "        y = [SOS_index] + y + [EOS_index]\n",
    "\n",
    "        return torch.LongTensor(x), len(x), torch.LongTensor(y), len(y)\n",
    "\n",
    "\n",
    "def custom_collate(data):\n",
    "    src = nn.utils.rnn.pad_sequence([sample[0] for sample in data], \n",
    "                                    padding_value=vocab.word2index[PAD_token])\n",
    "    src_lengths = torch.LongTensor([sample[1] for sample in data])\n",
    "    trg = nn.utils.rnn.pad_sequence([sample[2] for sample in data], \n",
    "                                    padding_value=vocab.word2index[PAD_token])\n",
    "    trg_lengths = torch.LongTensor([sample[3] for sample in data])\n",
    "    return src, src_lengths, trg, trg_lengths\n",
    "\n",
    "train_dataset = CustomDataset(SRC_train_dataset, TRG_train_dataset, vocab)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = CustomDataset(SRC_test_dataset, TRG_test_dataset, vocab)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                             collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder RNN iterates through the input sentence one token (e.g. word) at a time, at each time step outputting an *output* vector and a *hidden state* vector. The hidden state vector is then passed to the next time step, while the output vector is recorded. The encoder transforms the context it saw at each point in the sequence into a set of points in a high-dimensional space, which the decoder will use to generate a meaningful output for the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oQLTP2Wmi1eB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int, emb_size: int, hid_size: int, n_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_size = hid_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, emb_size)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_size, hid_size, n_layers,\n",
    "                           dropout=(0 if n_layers == 1 else dropout))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, input_lengths):\n",
    "        \"\"\"\n",
    "        src = [src len, batch size]\n",
    "        input_lengths = [batch size]\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src len, batch size, emb dim]\n",
    "\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, \n",
    "                                                   enforce_sorted=False)\n",
    "        # Forward pass through RNN\n",
    "        outputs, (hidden, cell) = self.rnn(packed)\n",
    "\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        # outputs = [src len, batch size, hid size * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hid size]\n",
    "        # cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder RNN generates the response sentence in a token-by-token fashion. It uses the encoder’s context vectors, and internal hidden states to generate the next word in the sequence. It continues generating words until it outputs an `EOS_token`, representing the end of the sentence. A common problem with a vanilla seq2seq decoder is that if we rely solely on the context vector to encode the entire input sequence’s meaning, it is likely that we will have information loss. This is especially the case when dealing with long input sequences, greatly limiting the capability of our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size: int, emb_size: int, hidden_size: int, n_layers: int=1, dropout: float=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, emb_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, last_cell, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "\n",
    "        # Forward through unidirectional RNN\n",
    "        rnn_output, (hidden, cell) = self.rnn(embedded, (last_hidden, last_cell))\n",
    "\n",
    "        # seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        # output = [1, batch size, hid dim]\n",
    "        # hidden = [n layers, batch size, hid dim]\n",
    "        # cell = [n layers, batch size, hid dim]\n",
    "\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        output = self.out(rnn_output)\n",
    "\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Combine them into a Seq2Seq Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle:\n",
    "\n",
    "* receiving the input/source sentence\n",
    "* using the encoder to produce the context vectors\n",
    "* using the decoder to produce the predicted output/target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, src_lengths, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        src = [src len, batch size]\n",
    "        trg = [trg len, batch size]\n",
    "        teacher_forcing_ratio is probability to use teacher forcing\n",
    "        e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_size\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # First input to the decoder is the SOS_tokens\n",
    "        decoder_input = trg[0].unsqueeze(0)\n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            # Insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            decoder_output, hidden, cell = self.decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            # Place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = decoder_output\n",
    "            \n",
    "            if self.training:\n",
    "                # Decide if we are going to use teacher forcing or not\n",
    "                teacher_force = random.random() < teacher_forcing_ratio\n",
    "                \n",
    "                # Get the highest predicted token from our predictions\n",
    "                top1 = decoder_output.argmax(1)\n",
    "\n",
    "                # If teacher forcing, use actual next token as next input\n",
    "                # if not, use predicted token\n",
    "                decoder_input = trg[t].unsqueeze(0) if teacher_force else top1.unsqueeze(0)\n",
    "            else:\n",
    "                top1 = decoder_output.argmax(1)\n",
    "                decoder_input = decoder_output.argmax(1).unsqueeze(0).detach()\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train & evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = vocab.n_words\n",
    "OUTPUT_SIZE = vocab.n_words\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "BIDIRECTIONAL = True\n",
    "N_LAYERS = 1\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "N_EPOCHS = 60\n",
    "CLIP = 1\n",
    "TEACHER_FORCING = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "MODEL_NAME = 'squad-lstm-chatbot.pt'\n",
    "\n",
    "enc = Encoder(INPUT_SIZE, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_SIZE, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is initializing the weights of our model. In the paper they state they initialize all weights from a uniform distribution between -0.08 and +0.08."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(3890, 256)\n",
       "    (rnn): LSTM(256, 512)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3890, 256)\n",
       "    (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (rnn): LSTM(256, 512)\n",
       "    (out): Linear(in_features=512, out_features=3890, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that will calculate the number of trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,141,170 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our optimizer, which we use to update our parameters in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(device)\n",
    "        src_lengths = batch[1]\n",
    "        trg = batch[2].to(device)\n",
    "        trg_lengths = batch[3]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_lengths, trg, TEACHER_FORCING)\n",
    "        \n",
    "        # trg = [trg len, batch size]\n",
    "        # output = [trg len, batch size, vocab size]\n",
    "              \n",
    "        vocab_size = output.shape[-1]\n",
    "        \n",
    "        output = output.view(-1, vocab_size)\n",
    "        trg = trg.view(-1)\n",
    "        \n",
    "        # output = [trg len * batch size, vocab size]\n",
    "        # trg = [trg len * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0].to(device)\n",
    "            src_lengths = batch[1]\n",
    "            trg = batch[2].to(device)\n",
    "            trg_lengths = batch[3]\n",
    "\n",
    "            output = model(src, src_lengths, trg, 0)  # Turn off teacher forcing\n",
    "\n",
    "            # trg = [trg len, batch size]\n",
    "            # output = [trg len, batch size, vocab size]\n",
    "\n",
    "            output_size = output.shape[-1]\n",
    "            \n",
    "            output = output.view(-1, output_size)\n",
    "            trg = trg.view(-1)\n",
    "\n",
    "            # trg = [trg len * batch size]\n",
    "            # output = [trg len * batch size, vocab size]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/60 | Time: 0m 48s\n",
      "\tTrain Loss: 4.320 | Train PPL:  75.205\n",
      "Epoch: 02/60 | Time: 0m 47s\n",
      "\tTrain Loss: 4.093 | Train PPL:  59.927\n",
      "Epoch: 03/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.973 | Train PPL:  53.139\n",
      "Epoch: 04/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.882 | Train PPL:  48.528\n",
      "Epoch: 05/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.799 | Train PPL:  44.672\n",
      "Epoch: 06/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.722 | Train PPL:  41.336\n",
      "Epoch: 07/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.644 | Train PPL:  38.243\n",
      "Epoch: 08/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.566 | Train PPL:  35.388\n",
      "Epoch: 09/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.490 | Train PPL:  32.795\n",
      "Epoch: 10/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.416 | Train PPL:  30.455\n",
      "Epoch: 11/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.346 | Train PPL:  28.385\n",
      "Epoch: 12/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.277 | Train PPL:  26.483\n",
      "Epoch: 13/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.207 | Train PPL:  24.700\n",
      "Epoch: 14/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.134 | Train PPL:  22.960\n",
      "Epoch: 15/60 | Time: 0m 47s\n",
      "\tTrain Loss: 3.064 | Train PPL:  21.423\n",
      "Epoch: 16/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.995 | Train PPL:  19.989\n",
      "Epoch: 17/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.925 | Train PPL:  18.633\n",
      "Epoch: 18/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.854 | Train PPL:  17.360\n",
      "Epoch: 19/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.785 | Train PPL:  16.201\n",
      "Epoch: 20/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.717 | Train PPL:  15.136\n",
      "Epoch: 21/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.644 | Train PPL:  14.067\n",
      "Epoch: 22/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.575 | Train PPL:  13.131\n",
      "Epoch: 23/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.504 | Train PPL:  12.233\n",
      "Epoch: 24/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.435 | Train PPL:  11.416\n",
      "Epoch: 25/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.364 | Train PPL:  10.632\n",
      "Epoch: 26/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.295 | Train PPL:   9.924\n",
      "Epoch: 27/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.227 | Train PPL:   9.270\n",
      "Epoch: 28/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.160 | Train PPL:   8.670\n",
      "Epoch: 29/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.090 | Train PPL:   8.082\n",
      "Epoch: 30/60 | Time: 0m 47s\n",
      "\tTrain Loss: 2.026 | Train PPL:   7.585\n",
      "Epoch: 31/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.963 | Train PPL:   7.118\n",
      "Epoch: 32/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.896 | Train PPL:   6.662\n",
      "Epoch: 33/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.835 | Train PPL:   6.262\n",
      "Epoch: 34/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.777 | Train PPL:   5.909\n",
      "Epoch: 35/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.718 | Train PPL:   5.573\n",
      "Epoch: 36/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.659 | Train PPL:   5.252\n",
      "Epoch: 37/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.602 | Train PPL:   4.962\n",
      "Epoch: 38/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.546 | Train PPL:   4.695\n",
      "Epoch: 39/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.493 | Train PPL:   4.451\n",
      "Epoch: 40/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.443 | Train PPL:   4.233\n",
      "Epoch: 41/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.392 | Train PPL:   4.021\n",
      "Epoch: 42/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.344 | Train PPL:   3.835\n",
      "Epoch: 43/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.295 | Train PPL:   3.653\n",
      "Epoch: 44/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.251 | Train PPL:   3.493\n",
      "Epoch: 45/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.207 | Train PPL:   3.343\n",
      "Epoch: 46/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.166 | Train PPL:   3.209\n",
      "Epoch: 47/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.122 | Train PPL:   3.072\n",
      "Epoch: 48/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.086 | Train PPL:   2.962\n",
      "Epoch: 49/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.848\n",
      "Epoch: 50/60 | Time: 0m 47s\n",
      "\tTrain Loss: 1.011 | Train PPL:   2.750\n",
      "Epoch: 51/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.976 | Train PPL:   2.655\n",
      "Epoch: 52/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.942 | Train PPL:   2.564\n",
      "Epoch: 53/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.909 | Train PPL:   2.482\n",
      "Epoch: 54/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.879 | Train PPL:   2.409\n",
      "Epoch: 55/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.850 | Train PPL:   2.340\n",
      "Epoch: 56/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.818 | Train PPL:   2.265\n",
      "Epoch: 57/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.792 | Train PPL:   2.209\n",
      "Epoch: 58/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.765 | Train PPL:   2.149\n",
      "Epoch: 59/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.740 | Train PPL:   2.095\n",
      "Epoch: 60/60 | Time: 0m 47s\n",
      "\tTrain Loss: 0.716 | Train PPL:   2.047\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, iter(train_dataloader), optimizer, criterion, CLIP)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Note that step should be called after evaluate()\n",
    "    # scheduler.step(valid_loss)\n",
    "        \n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model.state_dict(), MODEL_NAME)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.791 | Test PPL: 327.178 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_NAME))\n",
    "\n",
    "test_loss = evaluate(model, iter(test_dataloader), criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model file and compare some inference results with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(3890, 256)\n",
       "    (rnn): LSTM(256, 512)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(3890, 256)\n",
       "    (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (rnn): LSTM(256, 512)\n",
       "    (out): Linear(in_features=512, out_features=3890, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_NAME))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Which NFL team represented the AFC at Super Bowl 50?'\n",
      " 'Which NFL team represented the NFC at Super Bowl 50?'\n",
      " 'Where did Super Bowl 50 take place?' 'Which NFL team won Super Bowl 50?'\n",
      " 'What color was used to emphasize the 50th anniversary of the Super Bowl?'\n",
      " 'What was the theme of Super Bowl 50?' 'What day was the game played on?'\n",
      " 'What is the AFC short for?' 'What was the theme of Super Bowl 50?'\n",
      " 'What does AFC stand for?']\n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 963,  963, 1905,  963, 2420, 2420, 2420, 2420, 2420, 2420],\n",
      "        [ 234,  234, 1628,  234,  555,  623,  918,  439,  623, 2531],\n",
      "        [2266, 2266,  378, 2557,  623,    9,  623,    9,    9, 2743],\n",
      "        [   9,    9,  379,  378,  548, 2705,    9, 1412, 2705,   31],\n",
      "        [ 394,  394,  240,  379,   75,    7, 1381,   31,    7,    2],\n",
      "        [ 378,  378,  519,  240,    9,  378, 1087,    2,  378,    0],\n",
      "        [ 379,  379,   12,    2,    7,  379,  424,    0,  379,    0],\n",
      "        [ 240,  240,    2,    0,    9,  240,    2,    0,  240,    0],\n",
      "        [   2,    2,    0,    0,  378,    2,    0,    0,    2,    0],\n",
      "        [   0,    0,    0,    0,  379,    0,    0,    0,    0,    0],\n",
      "        [   0,    0,    0,    0,    2,    0,    0,    0,    0,    0]],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "questions = SRC_test_dataset[:10]\n",
    "print(questions)\n",
    "questions = [vocab.cleanText(q) for q in questions]\n",
    "questions = [[vocab.word2index[w] for w in question if w in vocab.word2index] for question in questions]\n",
    "questions = [torch.LongTensor([SOS_index] + question + [EOS_index]) for question in questions]\n",
    "question_lengths = [len(q) for q in questions]\n",
    "questions = nn.utils.rnn.pad_sequence(questions).to(device)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Denver Broncos',\n",
       " 'Carolina Panthers',\n",
       " 'Santa Clara, California',\n",
       " 'Denver Broncos',\n",
       " 'gold',\n",
       " '\"golden anniversary\"',\n",
       " 'February 7, 2016',\n",
       " 'American Football Conference',\n",
       " '\"golden anniversary\"',\n",
       " 'American Football Conference']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRG_test_dataset[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [ 375,  375,  416,  426,  337,   73, 2356,    9,   73, 1367],\n",
      "        [   2,    2,    2,    2,    2,    7,    2, 1978,    7,    2],\n",
      "        [   2,    2,    2,    2,    2,    9, 2245,  226,    9,    2],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,   57,    2,    2],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,   31,    2,    2],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,    9,    2,    2],\n",
      "        [   2,    2,    2,    9,    2,    2,    2, 3153,    2,    2],\n",
      "        [   2,    2,    2,  272,    2,    2,    2,    2,    2,    2],\n",
      "        [   2,    2,    2,  273,    2,    2,    2,    2,    2,    2],\n",
      "        [   2,    2,    2,  252,    2,    2,    2,    2,    2,    2],\n",
      "        [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2]],\n",
      "       device='cuda:1')\n",
      "['<sos> barcelona <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> barcelona <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> festiv <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> new <eos> <eos> <eos> <eos> <eos> the unit state citi <eos>',\n",
      " '<sos> black <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> top of the <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> friday <eos> 1985 <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> the crop research institut for the tropic <eos> <eos> <eos> <eos>',\n",
      " '<sos> top of the <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>',\n",
      " '<sos> digit <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>']\n"
     ]
    }
   ],
   "source": [
    "ANSWER_LENGTH = 12\n",
    "\n",
    "answers = torch.LongTensor(np.ones((ANSWER_LENGTH, len(questions[0])))) * SOS_index\n",
    "answers = model(questions, question_lengths, answers.to(device), 0)\n",
    "answers = torch.argmax(answers, axis=2)\n",
    "print(answers)\n",
    "answers = np.transpose([[vocab.index2word[int(w)] for w in answer] for answer in answers])\n",
    "answers = [' '.join(answer) for answer in answers]\n",
    "pprint(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interact with the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'STOP' to exit chat\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Hi, how are you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< with a glacier and repeat \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> So how have you been?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< a of of \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> I've been good. I'm in school right now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< includ develop of work \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> It's an ugly day today.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< the \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> I really wish it wasn't so hot every day.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< the second perform of \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> I can't wait until winter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< the the part of it 20th centuri \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> STOP\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'STOP' to exit chat\")\n",
    "ANSWER_LENGTH = 12\n",
    "\n",
    "while True:\n",
    "    inp = input(\">\")\n",
    "    \n",
    "    # If STOP in input, stop script\n",
    "    if \"STOP\" == inp.strip():\n",
    "        break\n",
    "    \n",
    "    # Quick error handling to account for any words not currently found in the vocabulary\n",
    "    try:\n",
    "        question = vocab.cleanText(inp)\n",
    "        question = [vocab.word2index[w] for w in question if w in vocab.word2index]\n",
    "        question = [torch.LongTensor([SOS_index] + question + [EOS_index])]\n",
    "        question_length = [len(q) for q in question]\n",
    "        question = nn.utils.rnn.pad_sequence(question).to(device)\n",
    "    except:\n",
    "        print(\"I could not understand this question. Please try again...\")\n",
    "        continue\n",
    "        \n",
    "    # Empty answer list for each word generated\n",
    "    answer = torch.LongTensor(np.ones((ANSWER_LENGTH, len(question[0])))) * SOS_index\n",
    "\n",
    "    # Get output - no target specified since not training\n",
    "    answer = model(question, question_length, answer.to(device), 0)\n",
    "    answer = torch.argmax(answer, axis=2)\n",
    "    \n",
    "    answer = np.transpose([[vocab.index2word[int(w)] for w in a] for a in answer])\n",
    "    eos_index = answer[0].tolist().index('<eos>')\n",
    "    answer = answer[0][1:eos_index]\n",
    "\n",
    "    # Finally, write out answer for user\n",
    "    print(\"<\", \" \".join(answer), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
